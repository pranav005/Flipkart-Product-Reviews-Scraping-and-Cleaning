{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDVXWWQDyfHu"
   },
   "source": [
    "### Import required libraries including NLTK and Reuters Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "nO887GAc2IlP",
    "outputId": "11e5ad9d-9c34-419a-b796-6ae055b26e42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters') # one time execution\n",
    "nltk.download('punkt')  # one time execution\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWGjl_py5yP5"
   },
   "source": [
    "### Know the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "_tnS_OL2bEvw",
    "outputId": "071ee503-3a58-4d51-9e2f-08ec1f10f1ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\\n  Mounting trade friction between the\\n  U.S. And Japan has raised fears among many of Asia's exporting\\n  nations that the row could inflict far-reachin\""
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the data\n",
    "reuters.raw()[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1T4e_ZpBy00z"
   },
   "outputs": [],
   "source": [
    "# get sentences\n",
    "dataset_sents = reuters.sents()\n",
    "# print first sentence\n",
    "dataset_sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Q63sKCJxy9gc",
    "outputId": "d2ad790f-205d-4c94-efa1-0847d59542c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54716"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of sentences\n",
    "len(dataset_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "6SgWoXcXzIcF",
    "outputId": "3de7fb8f-6523-4991-9f66-2207c0eb866e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN']"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of words\n",
    "dataset_words = reuters.words()\n",
    "# print first word\n",
    "dataset_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "24IEtnUAzeCc",
    "outputId": "be95dcfe-4091-4b28-ee79-9f37c54fc7ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1720901"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of words\n",
    "len(dataset_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "00Do0vfczlOb",
    "outputId": "4bc7f5c8-92e0-42a1-ade0-01b2e6aa6555"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41600"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of vocabulary\n",
    "len(set(dataset_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnXiiFQ759qH"
   },
   "source": [
    "### Split the Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reMY08314XMK"
   },
   "source": [
    "Divide the dataset into two Train data and Test data. Bigram Probabilities are learnt on the Train data and to evaluate it, we use the Test data. <br>\n",
    "Out of 54716 sentences, 40000 sentences are used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUiQqArqkwEQ"
   },
   "outputs": [],
   "source": [
    "data_sents = dataset_sents[:40000]\n",
    "data_sents_test = dataset_sents[40000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g65ozpPV5JSC"
   },
   "source": [
    "Get lists of tokens of train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VxKG3jG_0dsD",
    "outputId": "810138c2-affe-4b90-e7e7-fc541b9b6232"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1262448"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of words in train data\n",
    "num_words = 0\n",
    "for sentence in data_sents:\n",
    "  num_words += len(sentence)\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-rz7ZIO0iAN"
   },
   "outputs": [],
   "source": [
    "# create two lists containing words\n",
    "data_words_train = dataset_words[:num_words]\n",
    "data_words_test = dataset_words[num_words:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcSunRes6I8e"
   },
   "source": [
    "### Learn Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXbRsLiH6sk-"
   },
   "source": [
    "Method to generate a list of bigrams, and to get frequencies of unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vizo7IM-Mhxx"
   },
   "outputs": [],
   "source": [
    "def createBigram(data):\n",
    "\tlistOfBigrams = []\n",
    "\tbigramCounts = {}\n",
    "\tunigramCounts = {}\n",
    "\n",
    "\tfor i in range(len(data)):\n",
    "\t\tif i < len(data) - 1:\n",
    "\n",
    "\t\t\tlistOfBigrams.append((data[i], data[i + 1]))\n",
    "\n",
    "\t\t\tif (data[i], data[i+1]) in bigramCounts:\n",
    "\t\t\t\tbigramCounts[(data[i], data[i + 1])] += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tbigramCounts[(data[i], data[i + 1])] = 1\n",
    "\n",
    "\t\tif data[i] in unigramCounts:\n",
    "\t\t\tunigramCounts[data[i]] += 1\n",
    "\t\telse:\n",
    "\t\t\tunigramCounts[data[i]] = 1\n",
    "\n",
    "\treturn listOfBigrams, unigramCounts, bigramCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z87ncOlM7Qm-"
   },
   "source": [
    "Method to learn bigram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdMfz56Al2LX"
   },
   "outputs": [],
   "source": [
    "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "\n",
    "\tlistOfProb = {}\n",
    "\tfor bigram in listOfBigrams:\n",
    "\t\tword1 = bigram[0]\n",
    "\t\tword2 = bigram[1]\n",
    "\t\t\n",
    "\t\tlistOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n",
    "\n",
    "\tfile = open('bigramProb.txt', 'w')\n",
    "\tfile.write('Bigram' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n",
    "\n",
    "\tfor bigrams in listOfBigrams:\n",
    "\t\tfile.write(str(bigrams) + ' : ' + str(bigramCounts[bigrams]) + ' : ' + str(listOfProb[bigrams]) + '\\n')\n",
    "\tfile.close()\n",
    "\n",
    "\treturn listOfProb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25qCUF0N7YHd"
   },
   "source": [
    "Method to learn bigram probabilities with Add-1 Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9pe7LMO5exi"
   },
   "outputs": [],
   "source": [
    "def bigramWithAddOneSmoothing(listOfBigrams, unigramCounts, bigramCounts):\n",
    "\n",
    "\tlistOfProb = {}\n",
    "\tcStar = {}\n",
    "\n",
    "\n",
    "\tfor bigram in listOfBigrams:\n",
    "\t\tword1 = bigram[0]\n",
    "\t\tword2 = bigram[1]\n",
    "\t\tlistOfProb[bigram] = (bigramCounts.get(bigram) + 1)/(unigramCounts.get(word1) + len(unigramCounts))\n",
    "\t\tcStar[bigram] = (bigramCounts[bigram] + 1) * unigramCounts[word1] / (unigramCounts[word1] + len(unigramCounts))\n",
    "\n",
    "\tfile = open('addOneSmoothing.txt', 'w')\n",
    "\tfile.write('Bigram' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n",
    "\n",
    "\tfor bigrams in listOfBigrams:\n",
    "\t\tfile.write(str(bigrams) + ' : ' + str(bigramCounts[bigrams])\n",
    "\t\t\t\t   + ' : ' + str(listOfProb[bigrams]) + '\\n')\n",
    "\n",
    "\tfile.close()\n",
    "\n",
    "\treturn listOfProb, cStar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUqKF63arb6b"
   },
   "outputs": [],
   "source": [
    "# Main Program\n",
    "\n",
    "# Create a list of bigrams and get frequencies of unigrams and bigrams\n",
    "listOfBigrams, unigramCounts, bigramCounts = createBigram(data_words_train)\n",
    "\n",
    "# Calculate bigram probabilities\n",
    "bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
    "\n",
    "# Apply Add-1 Smoothing and calculate probabilities and get reconstructed count of bigrams\n",
    "bigramAddOne, addOneCstar = bigramWithAddOneSmoothing(listOfBigrams, unigramCounts, bigramCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ll7utCF6QRX"
   },
   "source": [
    "### Count Sentence Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY5gGr1D-78E"
   },
   "source": [
    "Input a sentence and generate bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGd4pxsOhiV7",
    "outputId": "0fe9f797-313b-4276-965c-4243cd7f3492"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('we', 'must'), ('must', 'be'), ('be', 'very'), ('very', 'careful')]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = 'we must be very careful'\n",
    "\n",
    "inputList = [] # list to store bigrams\n",
    "\n",
    "for i in range(len(input.split())-1):\n",
    "  inputList.append((input.split()[i], input.split()[i+1]))\n",
    "inputList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uIyvjaW_c2q"
   },
   "source": [
    "Get Bigram counts and probabilities use them to calculate probability of the input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GD5UlEA29PP0",
    "outputId": "0b557994-6873-45c4-b3c5-de7c5a1cd5e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8076159793430567e-07"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open a file to write output\n",
    "output1 = open('bigramProb-OUTPUT.txt', 'w')\n",
    "\n",
    "# initial probability of a sentence\n",
    "outputProb1 = 1\n",
    "\n",
    "output1.write('Bigram\\t\\t\\t\\t' + 'Count\\t\\t\\t\\t' + 'Probability\\n\\n')\n",
    "\n",
    "for i in range(len(inputList)):\n",
    "\n",
    "  # if bigram is present in the model, get updated probability\n",
    "  if inputList[i] in bigramProb:\n",
    "    # write bigram, its count and probability to the file\n",
    "    output1.write(str(inputList[i]) + '\\t\\t' + str(bigramCounts[inputList[i]]) + '\\t\\t' + str(bigramProb[inputList[i]]) + '\\n')\n",
    "    # multiply with probability of a current bigram\n",
    "    outputProb1 *= bigramProb[inputList[i]]\n",
    "\n",
    "  # if bigram is not present in the model, sentence probability is zero\n",
    "  else:\n",
    "    output1.write(str(inputList[i]) + '\\t\\t\\t' + str(0) + '\\t\\t\\t' + str(0) + '\\n')\n",
    "    outputProb1 *= 0\n",
    "\n",
    "output1.write('\\n' + 'Probablility = ' + str(outputProb1))\n",
    "outputProb1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btRgkXbDEOov"
   },
   "source": [
    "Use Add-1 Smoothed model to get the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmrxjZ899-tz",
    "outputId": "acbf82c5-4b1c-4f71-90b3-7b71fe3ee216"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2254340301928457e-14"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open a file to write output\n",
    "output2 = open('addOneSmoothing-OUTPUT.txt', 'w')\n",
    "\n",
    "# initial probability of a sentence\n",
    "outputProb2 = 1\n",
    "\n",
    "output2.write('Bigram\\t\\t\\t\\t' + 'Count\\t\\t\\t\\t' + 'Probability\\n\\n')\n",
    "\n",
    "for i in range(len(inputList)):\n",
    "\n",
    "  # if bigram is present in the model, get updated probability\n",
    "  if inputList[i] in bigramAddOne:\n",
    "    # Update probability of the sentence\n",
    "    outputProb2 *= bigramAddOne[inputList[i]]\n",
    "\n",
    "    output2.write(str(inputList[i]) + '\\t\\t' + str(addOneCstar[inputList[i]]) + '\\t\\t' + str(bigramAddOne[inputList[i]]) + '\\n')\n",
    "\n",
    "  # if bigram is not present in the model, use unigram counts to get estimated probability\n",
    "  else:\n",
    "    # if first word in a bigram is not present in unigrams, add with with count 1\n",
    "    if inputList[i][0] not in unigramCounts:\n",
    "      unigramCounts[inputList[i][0]] = 1\n",
    "    \n",
    "    # calculate probability of that word\n",
    "    prob = (1) / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n",
    "\n",
    "    # # reconstructed count for the bigram\n",
    "    addOneCStar = 1 * unigramCounts[inputList[i][0]] / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n",
    "    \n",
    "    # Update probability of the sentence\n",
    "    outputProb2 *= prob\n",
    "\n",
    "    output2.write(str(inputList[i]) + '\\t' + str(addOneCStar) + '\\t' + str(prob) + '\\n')\n",
    "\n",
    "output2.write('\\n' + 'Probablility = ' + str(outputProb2))\n",
    "outputProb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSSQl6WzEZw9"
   },
   "source": [
    "Show the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKLRvTgC-DeC",
    "outputId": "fb71dcac-1c29-4293-b569-d28fbf3a74bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we must be very careful\n",
      "[('we', 'must'), ('must', 'be'), ('be', 'very'), ('very', 'careful')]\n",
      "Bigram Model:  2.8076159793430567e-07\n",
      "Add One:  4.2254340301928457e-14\n"
     ]
    }
   ],
   "source": [
    "# input sentence\n",
    "print(input)\n",
    "\n",
    "# list of bigrams\n",
    "print(inputList)\n",
    "\n",
    "# probability given by simple bigram model\n",
    "print ('Bigram Model: ', outputProb1)\n",
    "\n",
    "# probability given by bigram model with add-1 smoothing\n",
    "print ('Add One: ', outputProb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnoVrDtQKSBX"
   },
   "source": [
    "### Next Word Recommandation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulm4Oo55ZH3-"
   },
   "outputs": [],
   "source": [
    "def sentence_prob_with_next_word(next_word):\n",
    "  outputProb = 1\n",
    "  new_bigram = (input.split()[-1], next_word)\n",
    "  if new_bigram in bigramAddOne:\n",
    "    outputProb *= bigramAddOne[new_bigram]\n",
    "  else:\n",
    "    if new_bigram[0] not in unigramCounts:\n",
    "      unigramCounts[new_bigram[0]] = 1\n",
    "    prob = (1) / (unigramCounts[new_bigram[0]] + len(unigramCounts))\n",
    "    outputProb *= prob\n",
    "  return outputProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RI-D6IgAR7L0"
   },
   "outputs": [],
   "source": [
    "input = 'the investors are'\n",
    "possible_words = ['cheated', 'happy', 'smart', 'afraid']\n",
    "\n",
    "inputList = []\n",
    "outputProb = 1\n",
    "\n",
    "for i in range(len(input.split())-1):\n",
    "  inputList.append((input.split()[i], input.split()[i+1]))\n",
    "\n",
    "\n",
    "for i in range(len(inputList)):\n",
    "\n",
    "  if inputList[i] in bigramAddOne:\n",
    "    outputProb *= bigramAddOne[inputList[i]]\n",
    "  else:\n",
    "    if inputList[i][0] not in unigramCounts:\n",
    "      unigramCounts[inputList[i][0]] = 1\n",
    "    prob = (1) / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n",
    "    outputProb *= prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQP7k6ZhKlZw",
    "outputId": "53947bec-d7f1-43d5-a368-6bd2f4bea899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Word: happy\n",
      "Output Sentece: the investors are happy\n"
     ]
    }
   ],
   "source": [
    "max_prob = 0\n",
    "index_of_next_word = -1\n",
    "for i, word in enumerate(possible_words):\n",
    "  final_prob = outputProb * sentence_prob_with_next_word(word)\n",
    "  if final_prob > max_prob:\n",
    "    max_prob = final_prob\n",
    "    index_of_next_word = i\n",
    "\n",
    "print('Next Word:', possible_words[index_of_next_word])\n",
    "print('Output Sentece:', input, possible_words[index_of_next_word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wvpm9Wnv-auQ"
   },
   "source": [
    "### Word Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxHSAzEx-ekP",
    "outputId": "ae1c2ee6-6714-4c41-ceb9-47b8b2dce7e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the market is very happy these days : 3.0787233025784113e-22\n",
      "market is the happy these very days : 2.5840603259051406e-24\n"
     ]
    }
   ],
   "source": [
    "input1 = 'the market is very happy these days'\n",
    "input2 = 'market is the happy these very days'\n",
    "\n",
    "\n",
    "inputList1 = []\n",
    "inputList2 = []\n",
    "\n",
    "\n",
    "outputProb1 = 1\n",
    "outputProb2 = 1\n",
    "\n",
    "\n",
    "for i in range(len(input1.split())-1):\n",
    "  inputList1.append((input1.split()[i], input1.split()[i+1]))\n",
    "\n",
    "for i in range(len(input2.split())-1):\n",
    "  inputList2.append((input2.split()[i], input2.split()[i+1]))\n",
    "\n",
    "\n",
    "for i in range(len(inputList1)):\n",
    "  if inputList1[i] in bigramAddOne:\n",
    "    outputProb1 *= bigramAddOne[inputList1[i]]\n",
    "  else:\n",
    "    if inputList1[i][0] not in unigramCounts:\n",
    "      unigramCounts[inputList1[i][0]] = 1\n",
    "    prob1 = (1) / (unigramCounts[inputList1[i][0]] + len(unigramCounts))\n",
    "    outputProb1 *= prob1\n",
    "\n",
    "\n",
    "for i in range(len(inputList2)):\n",
    "  if inputList2[i] in bigramAddOne:\n",
    "    outputProb2 *= bigramAddOne[inputList2[i]]\n",
    "  else:\n",
    "    if inputList2[i][0] not in unigramCounts:\n",
    "      unigramCounts[inputList2[i][0]] = 1\n",
    "    prob2 = (1) / (unigramCounts[inputList2[i][0]] + len(unigramCounts))\n",
    "    outputProb2 *= prob2\n",
    "\n",
    "print (input1, ':', outputProb1)\n",
    "print (input2, ':', outputProb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xRmdeXSE3eX"
   },
   "source": [
    "### Sentence with Homonyms: piece or peace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NEjoIYOGA61"
   },
   "source": [
    "Bigram model with add-1 smoothing is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vav3BVn3F8w_",
    "outputId": "261a66e1-190c-4183-a051-5d3db6041167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its a peace of information : 8.014242631151717e-19\n",
      "its a piece of information : 3.2057856418889864e-18\n"
     ]
    }
   ],
   "source": [
    "input1 = 'its a peace of information'\n",
    "input2 = 'its a piece of information'\n",
    "\n",
    "\n",
    "inputList1 = []\n",
    "inputList2 = []\n",
    "\n",
    "\n",
    "outputProb1 = 1\n",
    "outputProb2 = 1\n",
    "\n",
    "\n",
    "for i in range(len(input1.split())-1):\n",
    "  inputList1.append((input1.split()[i], input1.split()[i+1]))\n",
    "\n",
    "for i in range(len(input2.split())-1):\n",
    "  inputList2.append((input2.split()[i], input2.split()[i+1]))\n",
    "\n",
    "\n",
    "for i in range(len(inputList1)):\n",
    "  if inputList1[i] in bigramAddOne:\n",
    "    outputProb1 *= bigramAddOne[inputList1[i]]\n",
    "  else:\n",
    "    if inputList1[i][0] not in unigramCounts:\n",
    "      unigramCounts[inputList1[i][0]] = 1\n",
    "    prob1 = (1) / (unigramCounts[inputList1[i][0]] + len(unigramCounts))\n",
    "    outputProb1 *= prob1\n",
    "\n",
    "\n",
    "for i in range(len(inputList2)):\n",
    "  if inputList2[i] in bigramAddOne:\n",
    "    outputProb2 *= bigramAddOne[inputList2[i]]\n",
    "  else:\n",
    "    if inputList2[i][0] not in unigramCounts:\n",
    "      unigramCounts[inputList2[i][0]] = 1\n",
    "    prob2 = (1) / (unigramCounts[inputList2[i][0]] + len(unigramCounts))\n",
    "    outputProb2 *= prob2\n",
    "\n",
    "print (input1, ':', outputProb1)\n",
    "print (input2, ':', outputProb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PfddoybIYFa"
   },
   "source": [
    "### Perplexity of Model <br>\n",
    "Perplexity of add-1 smoothed bigram model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cmxsw-fwIg5Y"
   },
   "source": [
    "Method to get probability of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBac7Q0Sg7pG"
   },
   "outputs": [],
   "source": [
    "def calculate_bigram_sentence_probability(input):\n",
    "\n",
    "  inputList = []\n",
    "  outputProb = 1\n",
    "\n",
    "  for i in range(len(input)-1):\n",
    "    inputList.append((input[i], input[i+1]))\n",
    "\n",
    "  for i in range(len(inputList)):\n",
    "    if inputList[i] in bigramAddOne:\n",
    "      outputProb *= bigramAddOne[inputList[i]]\n",
    "    else:\n",
    "      if inputList[i][0] not in unigramCounts:\n",
    "        unigramCounts[inputList[i][0]] = 1\n",
    "      prob = (1) / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n",
    "      outputProb *= prob\n",
    "\n",
    "  return outputProb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8fv-8-kJLHB"
   },
   "source": [
    "Method to get total number of bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9i2rXyhQiU-"
   },
   "outputs": [],
   "source": [
    "def calculate_number_of_bigrams(sentences):\n",
    "        bigram_count = 0\n",
    "        for sentence in sentences:\n",
    "            # remove one for number of bigrams in sentence\n",
    "            bigram_count += len(sentence) - 1\n",
    "        return bigram_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAlU--jrJVna"
   },
   "source": [
    "Method to calculate perplexity of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoUq8FC7HtT8"
   },
   "outputs": [],
   "source": [
    "def calculate_bigram_perplexity(model, sentences):\n",
    "    number_of_bigrams = calculate_number_of_bigrams(sentences)\n",
    "    bigram_sentence_probability_log_sum = 0\n",
    "    for sentence in sentences:\n",
    "        p = calculate_bigram_sentence_probability(sentence)\n",
    "        if p != 0.0:\n",
    "          a = math.log(p)\n",
    "        else:\n",
    "          a = 0\n",
    "        bigram_sentence_probability_log_sum -= a\n",
    "    return math.pow(2, bigram_sentence_probability_log_sum / number_of_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmwFEZIhJu4Q"
   },
   "source": [
    "Perplexity of the model over train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydUwPXUMNQ6y",
    "outputId": "432aeebf-8c46-4c63-c090-b53550f7f3c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERPLEXITY over Training Data: 137.07939217258775\n",
      "PERPLEXITY over Test Data: 168.93416630740222\n"
     ]
    }
   ],
   "source": [
    "print(\"PERPLEXITY over Training Data:\", calculate_bigram_perplexity(bigramAddOne, data_sents))\n",
    "print(\"PERPLEXITY over Test Data:\", calculate_bigram_perplexity(bigramAddOne, data_sents_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7iZZDMLwIGN"
   },
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCsjMMDu4z9v"
   },
   "source": [
    "Text Generation Using Simple Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Rm88ndODxEsU",
    "outputId": "867afe69-6bc7-485a-8a35-fda8ce3fa6f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is expected within Europe and a free areas in January , 470 tonnes , 682 Avg shrs 30 Oper net\n"
     ]
    }
   ],
   "source": [
    "# initial word\n",
    "text = [\"there\"]\n",
    "\n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "  # select a random probability threshold  \n",
    "  r = random.random()\n",
    "  accumulator = 0.0\n",
    "\n",
    "  for pair in bigramProb.keys():\n",
    "    if pair[0] == text[-1]:\n",
    "      accumulator += bigramProb[pair]\n",
    "      # select words that are above the probability threshold\n",
    "      if accumulator >= r:\n",
    "          text.append(pair[1])\n",
    "          break\n",
    "\n",
    "  if text[-1] == 'None':\n",
    "    sentence_finished = True\n",
    "  if len(text) > 20:\n",
    "    sentence_finished = True\n",
    " \n",
    "print (' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yUBX2Db48aH"
   },
   "source": [
    "Text Generation Using Simple Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "id": "Bhw3fGeX2cvA",
    "outputId": "94ad1c0f-1720-4f0d-d886-7a32fc4ccc87"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-51f0e340f8fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0maccumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mtpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrigramProb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtpl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtpl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0maccumulator\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrigramProb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtpl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trigramProb' is not defined"
     ]
    }
   ],
   "source": [
    "# initial words\n",
    "text = [\"there\", \"is\"]\n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "  # select a random probability threshold  \n",
    "  r = random.random()\n",
    "  accumulator = 0.0\n",
    "\n",
    "  for tpl in trigramProb.keys():\n",
    "    if (tpl[0] == text[-2] and tpl[1] == text[-1]):\n",
    "      accumulator += trigramProb[tpl]\n",
    "      # select words that are above the probability threshold\n",
    "      if accumulator >= r:\n",
    "          text.append(tpl[2])\n",
    "          break\n",
    "\n",
    "  if text[-2:] == ['None', 'None']:\n",
    "    sentence_finished = True\n",
    "  if len(text) > 20:\n",
    "    sentence_finished = True\n",
    " \n",
    "print (' '.join([t for t in text if t]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3xRmdeXSE3eX",
    "2PfddoybIYFa",
    "LEv3fjcz_iZi"
   ],
   "name": "BigramLM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
